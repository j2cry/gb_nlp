{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: cuda\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from functools import partial\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Selected device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\"Родился на улице Герцена, в гастрономе номер двадцать два. Известный экономист, по призванию своему — библиотекарь. В народе — колхозник. В магазине — продавец.\n",
    "    В экономике, так сказать, необходим. Это, так сказать, система… э-э-э… в составе ста двадцати единиц. Фотографируете Мурманский полуостров и получаете «Те-ле-фун-кен».\n",
    "    И бухгалтер работает по другой линии — по линии библиотекаря. Потому что не воздух будет, академик будет! Ну вот можно сфотографировать Мурманский полуостров. Можно\n",
    "    стать воздушным асом. Можно стать воздушной планетой. И будешь уверен, что эту планету примут по учебнику. Значит, на пользу физике пойдёт одна планета. Величина,\n",
    "    оторванная в область дипломатии, даёт свои колебания на всю дипломатию. А Илья Муромец даёт колебания только на семью на свою. Спичка в библиотеке работает. В\n",
    "    кинохронику ходят и зажигают в кинохронике большой лист. В библиотеке маленький лист разжигают. Огонь… э-э-э… будет вырабатываться гораздо легче, чем учебник крепкий.\n",
    "    А крепкий учебник будет весомее, чем гастроном на улице Герцена. А на улице Герцена будет расщеплённый учебник. Тогда учебник будет проходить через улицу Герцена,\n",
    "    через гастроном номер двадцать два, и замещаться там по формуле экономического единства. Вот в магазине двадцать два она может расщепиться, экономика! На экономистов,\n",
    "    на диспетчеров, на продавцов, на культуру торговли… Так что, в эту сторону двинется вся экономика. Библиотека двинется в сторону ста двадцати единиц, которые будут…\n",
    "    э-э-э… предмет укладывать на предмет. Сто двадцать единиц — предмет физика. Электрическая лампочка горит от ста двадцати кирпичей, потому что структура, так сказать,\n",
    "    похожа у неё на кирпич. Илья Муромец работает на стадионе «Динамо». Илья Муромец работает у себя дома. Вот конкретная дипломатия! Открытая дипломатия — то же самое.\n",
    "    Ну, берём телевизор, вставляем в Мурманский полуостров, накручиваем там… э-э-э… всё время чёрный хлеб… Так что же, будет Муромец, что ли, вырастать? Илья Муромец,\n",
    "    что ли, будет вырастать из этого?\"\"\"\n",
    "data = re.sub('\\n', ' ', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dictionaries\n",
    "idx2sym = dict(enumerate(set(data)))\n",
    "sym2idx = {v: k for k, v in idx2sym.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymbolDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text, window=8):\n",
    "        self.length = len(text) - window + 1\n",
    "        self.x = [[sym2idx[s] for s in text[i:i+window]] for i in range(self.length - 1)]\n",
    "        self.y = [[sym2idx[s] for s in text[i:i+window]] for i in range(1, self.length)]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.as_tensor(self.x[index]), torch.as_tensor(self.y[index])\n",
    "        # return torch.as_tensor(self.x[index]), torch.as_tensor(np.eye(len(sym2idx))[self.y[index]])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length - 1\n",
    "\n",
    "\n",
    "TEST_FRAC = 0.2\n",
    "WINDOW = 144\n",
    "BATCH_SIZE = 128 \n",
    "# make datasets\n",
    "bound = math.ceil(len(data) * (1 - TEST_FRAC))\n",
    "\n",
    "train_dataset = SymbolDataset(data[:bound], window=WINDOW)\n",
    "valid_dataset = SymbolDataset(data[bound:], window=WINDOW)\n",
    "# make loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainable:\n",
    "    def fit(self, loader, optim, crit, *, epochs=5, dev='cpu', eval_params=None):\n",
    "        \"\"\"\n",
    "        :param loader - data loader\n",
    "        :param optim - optimizer\n",
    "        :param crit - criterion\n",
    "        :param epochs\n",
    "        :param device\n",
    "        :param eval_params - predict() parameters for evaluation\n",
    "        \"\"\"\n",
    "        self.crit = crit\n",
    "        self.dev = dev\n",
    "        if not eval_params:\n",
    "            eval_params = dict()\n",
    "        self.train()\n",
    "        \n",
    "        for ep in range(epochs):\n",
    "            sum_loss, items = 0.0, 0\n",
    "            pbar = tqdm(enumerate(loader), total=len(loader), desc=f'Epoch {ep + 1}/{epochs}')\n",
    "            for i, batch in pbar:\n",
    "                inputs, labels = batch[0].to(dev), batch[1].to(dev)\n",
    "                optim.zero_grad()\n",
    "                outputs = self(inputs)\n",
    "                # print(outputs.shape, labels.shape)\n",
    "                # print(outputs)\n",
    "                # print(labels)\n",
    "                loss = crit(outputs, labels)\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "\n",
    "                sum_loss += loss.item()\n",
    "                items += len(labels)\n",
    "                pbar.set_postfix({'cumulative loss per item': sum_loss / items})\n",
    "\n",
    "                # evaluate\n",
    "                if (i + 1 == len(loader)) and eval_params:\n",
    "                    self.eval()\n",
    "                    valid_loader = eval_params.get('valid_loader')\n",
    "                    pbar.set_postfix_str('calculating final loss...')\n",
    "\n",
    "                    train_result = self.predict_(loader, **eval_params)\n",
    "                    report = {'loss': train_result[1], 'metric': 'n/a' if len(train_result) == 2 else train_result[2]}\n",
    "\n",
    "                    if valid_loader is not None:\n",
    "                        valid_result = self.predict_(valid_loader, **eval_params)\n",
    "                        report = {'loss': f'{train_result[1]:03f}/{valid_result[1]:03f}',\n",
    "                                  'metric': 'n/a' if len(train_result) == 2 else f'{train_result[2]:03f}/{valid_result[2]:03f}'}\n",
    "                    pbar.set_postfix(report)\n",
    "                    self.train()\n",
    "        print('\\nDone.')\n",
    "\n",
    "    def predict_(self, loader, *, metric=None, threshold=0.5, **kwargs):\n",
    "        if not hasattr(self, 'dev'):\n",
    "            raise AttributeError('Model is not trained.')\n",
    "        self.eval()\n",
    "        loss = 0\n",
    "        for i, batch in enumerate(loader):\n",
    "            inputs, labels = batch[0].to(self.dev), batch[1].to(self.dev)\n",
    "            outputs = self(inputs)\n",
    "            predicts = torch.cat([predicts, outputs]) if i > 0 else outputs\n",
    "            true_labels = torch.cat([true_labels, labels]) if i > 0 else labels\n",
    "\n",
    "            loss += self.crit(outputs, labels).item() / len(loader)\n",
    "        result = [predicts, loss]\n",
    "        if metric:\n",
    "            # pred_labels = (predicts > threshold) * 1            \n",
    "            result.append(metric(true_labels.cpu(), predicts.detach().cpu()))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module, Trainable):\n",
    "    def __init__(self, dict_size, embedding_dim=128, drop=0.3, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding = torch.nn.Embedding(dict_size, embedding_dim)\n",
    "        self.lstm_1 = torch.nn.GRU(embedding_dim, 2 * embedding_dim, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        # self.lstm_1 = torch.nn.LSTM(embedding_dim, 2 * embedding_dim, num_layers=2, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.linear = torch.nn.Linear(2 * embedding_dim, num_classes)\n",
    "        self.dp = torch.nn.Dropout(drop)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, h = self.lstm_1(x)\n",
    "        x = torch.max_pool1d(x, 2)\n",
    "        x = self.dp(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.linear(x)\n",
    "        # x = torch.softmax(x, dim=2)       # по идее же оно тут должно быть - ибо классификация, но с ним обучается ооочень медленно\n",
    "        return x.permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 12/12 [00:05<00:00,  2.37it/s, loss=2.169623/2.503678, metric=0.233649/0.201345]\n",
      "Epoch 2/5: 100%|██████████| 12/12 [00:05<00:00,  2.37it/s, loss=0.800819/1.225383, metric=0.640865/0.516055]\n",
      "Epoch 3/5: 100%|██████████| 12/12 [00:05<00:00,  2.37it/s, loss=0.251934/0.611067, metric=0.886235/0.648403]\n",
      "Epoch 4/5: 100%|██████████| 12/12 [00:05<00:00,  2.39it/s, loss=0.105799/0.428143, metric=0.947028/0.724569]\n",
      "Epoch 5/5: 100%|██████████| 12/12 [00:05<00:00,  2.37it/s, loss=0.056578/0.362542, metric=0.982380/0.779703]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DICT_SIZE = len(sym2idx)\n",
    "\n",
    "torch.manual_seed(11)       # just in case\n",
    "model = Net(dict_size=DICT_SIZE, embedding_dim=256, drop=0.05, num_classes=DICT_SIZE).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def metric(true_val, pred_val, mfunc=accuracy_score, **kwargs):\n",
    "    values = list(map(partial(mfunc, **kwargs), true_val, pred_val.argmax(dim=1)))\n",
    "    return np.mean(values)\n",
    "\n",
    "eval_params = {\n",
    "    'softmax': True,\n",
    "    'metric': partial(metric, mfunc=f1_score, average='macro'),\n",
    "    'valid_loader': valid_loader\n",
    "}\n",
    "model.fit(train_loader, optimizer, criterion, epochs=5, dev=device, eval_params=eval_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' потому что структура, так сказать,     похожа у не, на кирпик. Илья Муромец работает на стадионе  финамой. Илья Муромец работает у себя дома. Вот конкретная дипломатий  сткрытая дипломатия — то Ге ссмое.     Ну, бермм телевизор, вставльем в Мурманский полуостров, н'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts = model.predict_(valid_loader)\n",
    "detached = predicts[0].detach().cpu()\n",
    "''.join([idx2sym[i] for i in detached.argmax(dim=1)[:, 0].numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "23a8a6843721b26098060b435da282c6499d0f0384483463012990926fcfc80c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
