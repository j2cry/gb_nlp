{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/avagadro/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/avagadro/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pathlib\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from joblib import parallel_backend, Parallel, delayed\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# prepare environment\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "DATA = pathlib.Path('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У меня Fedora и GTX1070, tensorflow из коробки с cuda не работает, видимо надо собирать из исходников.\n",
    "Чтобы этого не делать, взял предобученную сетку  с huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avagadro/anaconda3/envs/jupyter_default/lib/python3.9/site-packages/huggingface_hub/file_download.py:621: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-25 22:22:37,526 loading file /home/avagadro/.flair/models/ner-english-ontonotes-fast/0d55dd3b912da9cf26e003035a0c269a0e9ab222f0be1e48a3bbba3a58c0fed0.c9907cd5fde3ce84b71a4172e7ca03841cd81ab71d13eb68aa08b259f57c00b6\n",
      "2022-10-25 22:22:42,906 SequenceTagger predicts: Dictionary with 76 tags: <unk>, O, B-CARDINAL, E-CARDINAL, S-PERSON, S-CARDINAL, S-PRODUCT, B-PRODUCT, I-PRODUCT, E-PRODUCT, B-WORK_OF_ART, I-WORK_OF_ART, E-WORK_OF_ART, B-PERSON, E-PERSON, S-GPE, B-DATE, I-DATE, E-DATE, S-ORDINAL, S-LANGUAGE, I-PERSON, S-EVENT, S-DATE, B-QUANTITY, E-QUANTITY, S-TIME, B-TIME, I-TIME, E-TIME, B-GPE, E-GPE, S-ORG, I-GPE, S-NORP, B-FAC, I-FAC, E-FAC, B-NORP, E-NORP, S-PERCENT, B-ORG, E-ORG, B-LANGUAGE, E-LANGUAGE, I-CARDINAL, I-ORG, S-WORK_OF_ART, I-QUANTITY, B-MONEY\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "# load tagger\n",
    "tagger = SequenceTagger.load(\"flair/ner-english-ontonotes-fast\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__load data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv(DATA / 'train.tsv', sep='\\t')\n",
    "valid_raw = pd.read_csv(DATA / 'test.tsv', sep='\\t')\n",
    "\n",
    "# concat\n",
    "raw = pd.concat([train_raw, valid_raw])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__predict NER-tags__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare corpus\n",
    "cleaned = raw['review'].str.replace(r'<.*?>', '', regex=True)       # remove html tags\n",
    "with parallel_backend('loky'):\n",
    "    corpus = Parallel()(delayed(Sentence)(elem) for elem in cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tagging: 100%|██████████| 2000/2000 [1:14:40<00:00,  2.24s/it]  \n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 25\n",
    "rng = range(np.ceil(len(corpus) / BATCH_SIZE).astype(int))\n",
    "\n",
    "for i in tqdm(rng, total=len(rng), desc='tagging'):\n",
    "    tagger.predict(corpus[i*BATCH_SIZE:(i+1)*BATCH_SIZE])\n",
    "\n",
    "# save\n",
    "pickle.dump(corpus, open((DATA / 'corpus.pkl').as_posix(), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__replace tokens__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "corpus = pickle.load(open((DATA / 'corpus.pkl').as_posix(), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "replacing NERs: 100%|██████████| 50000/50000 [05:05<00:00, 163.70it/s]\n"
     ]
    }
   ],
   "source": [
    "REPLACE_NER = ['ORG', 'PERSON', 'FAC', 'GPE', 'LOC']    # эти сущности заменяются на соотв. теги\n",
    "# https://huggingface.co/flair/ner-english-ontonotes-fast?text=On+September+1st+George+Washington+won+1+dollar.\n",
    "\n",
    "def replace_tokens(sentence, as_list=True):\n",
    "    sn_tokens = []\n",
    "    for token in sentence.tokens:\n",
    "        tk = token.text\n",
    "        for span in sentence.get_spans('ner'):\n",
    "            label = span.get_label().value\n",
    "            if (token in span.tokens) and (label in REPLACE_NER):\n",
    "                tk = f'[{label}]'\n",
    "                break\n",
    "        sn_tokens.append(tk)\n",
    "    return sn_tokens if as_list else ' '.join(sn_tokens)\n",
    "\n",
    "\n",
    "with parallel_backend('loky'):\n",
    "    replaced = Parallel()(delayed(replace_tokens)(sentence) for sentence in tqdm(corpus, desc='replacing NERs'))\n",
    "\n",
    "# save\n",
    "pickle.dump(replaced, open((DATA / 'corpus_replaced.pkl').as_posix(), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "replaced = pickle.load(open((DATA / 'corpus_replaced.pkl').as_posix(), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = nltk.corpus.stopwords.words('english')\n",
    "lemfunc = np.vectorize(WordNetLemmatizer().lemmatize)   # векторизация лемматизатора для ускорения работы со списками\n",
    "\n",
    "def drop_stopwords(tokens, min_length=3):\n",
    "    return [w.lower() for w in tokens if w not in STOPWORDS and len(w) >= min_length]\n",
    "\n",
    "\n",
    "# lemmatize\n",
    "with parallel_backend('loky'):\n",
    "    prepared = Parallel()(delayed(lemfunc)(elem) for elem in replaced)\n",
    "# drop stopwords\n",
    "with parallel_backend('loky'):\n",
    "    prepared = Parallel()(delayed(drop_stopwords)(elem) for elem in prepared)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# back split and save\n",
    "data = pd.concat([pd.Series(prepared), raw['is_positive'].reset_index(drop=True)], axis=1).rename(columns={0: 'tokens'})\n",
    "\n",
    "data[:train_raw.shape[0]].to_csv(DATA / 'prepared_train.csv', index=False)\n",
    "data[train_raw.shape[0]:].to_csv(DATA / 'prepared_valid.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('jupyter_default')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "23a8a6843721b26098060b435da282c6499d0f0384483463012990926fcfc80c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
