{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import FastText\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "DATA = pathlib.Path('data') / 'coll'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__load data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(dir):\n",
    "    texts = []\n",
    "    spans = []\n",
    "    files = list(set(f.split('.')[0] for f in os.listdir(dir)))\n",
    "    for f in tqdm(files, desc='loading'):\n",
    "        txt = open(DATA.joinpath(f + '.txt').as_posix(), 'r', encoding='utf-8').read()\n",
    "        ann = open(DATA.joinpath(f + '.ann').as_posix(), 'r', encoding='utf-8').readlines()\n",
    "        ann = [item.replace(u'\\t' , ' ').strip().split(maxsplit=4) for item in ann]\n",
    "        sp = [[idx, tag, int(start), int(stop), text] for idx, tag, start, stop, text in ann]\n",
    "        texts.append(txt)\n",
    "        spans.append(sp)\n",
    "    return texts, spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading: 100%|██████████| 1000/1000 [00:00<00:00, 4123.54it/s]\n"
     ]
    }
   ],
   "source": [
    "texts, spans = loader(DATA.as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Министра</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Белуджистана</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>отправили</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>в</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>отставку</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sent          word  tag\n",
       "0     0      Министра  OUT\n",
       "1     0  Белуджистана  LOC\n",
       "2     0     отправили  OUT\n",
       "3     0             в  OUT\n",
       "4     0      отставку  OUT"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build dataframe\n",
    "docs = []\n",
    "for ix, (txt, ann) in enumerate(zip(texts, spans)):\n",
    "    words = []\n",
    "    for token in word_tokenize(txt, language='russian'):\n",
    "        tag = 'OUT'\n",
    "        for item in ann:\n",
    "            if txt[item[2]:item[3]] == token == item[4]:    # если токен в тексте совпадает с токеном разметки\n",
    "                tag = item[1]\n",
    "                break\n",
    "        words.append([ix, token, tag])\n",
    "    docs.extend(words)\n",
    "\n",
    "data = pd.DataFrame(docs, columns=['sent', 'word', 'tag'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OUT         255273\n",
       "ORG           1397\n",
       "PER           1356\n",
       "GEOPOLIT      1188\n",
       "LOC            468\n",
       "MEDIA           62\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tag'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__natasha NER__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import Segmenter, MorphVocab, NewsEmbedding, NewsNERTagger, Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare\n",
    "segmenter = Segmenter()\n",
    "emb = NewsEmbedding()\n",
    "ner_tagger = NewsNERTagger(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Министра Белуджистана отправили в отставку из-за терактов\n",
      "         PER─────────                                    \n",
      "Премьер-министр Пакистана Раджа Первез Ашраф 14 января отправил в \n",
      "                LOC────── PER───────────────                      \n",
      "отставку главного министра провинции Белуджистан, где 10 января в \n",
      "                                     LOC────────                  \n",
      "результате серии терактов погибли не менее 115 человек. Об этом \n",
      "сообщает Agence France-Presse.\n",
      "         ORG───────────────── \n",
      "Такое решение премьер-министр принял после встречи в городе Кветта с \n",
      "                                                            LOC───   \n",
      "мусульманами-шиитами, протестующими против бездействия властей в сфере\n",
      " безопасности. Как заявил премьер-министр, в течение следующих двух \n",
      "месяцев после отставки министра Белуджистана контролировать провинцию \n",
      "                                PER─────────                          \n",
      "будет специально назначенный губернатор.\n",
      "Кроме того, премьер-министр согласился выполнить требование \n",
      "родственников погибших о введении контроля вооруженных сил над \n",
      "администрацией и службами безопасности города Кветта.\n",
      "                                              LOC─── \n",
      "Как сообщалось ранее, 11 января, родственники мусульман-шиитов, \n",
      "погибших во время терактов, отказались хоронить их до тех пор, пока \n",
      "власти страны не предпримут действий для обеспечения безопасности \n",
      "жителей и не выполнят ряд других требований. В городе также прошли \n",
      "массовые митинги шиитов.\n",
      "Вечером 10 января в городе Кветта в пакистанской провинции Белуджистан\n",
      "                           LOC───                          LOC────────\n",
      " террорист-смертник взорвался внутри бильярдного клуба. Это привело к \n",
      "обрушению здания и большому количеству жертв. После того, как на месте\n",
      " происшествия собрались полицейские, врачи, журналисты и другие люди, \n",
      "произошел взрыв припаркованной рядом машины, в результате чего число \n",
      "погибших стало еще больше. В организации взрывов призналась \n",
      "радикальная суннитская группировка \"Лашкар-и-Тайба\".\n",
      "                                    ORG───────────  \n",
      "В тот же день в Кветте произошел еще один теракт, за который взяла на \n",
      "                LOC───                                                \n",
      "себя ответственность сепаратистская группировка \"Объединенная армия \n",
      "                                                 ORG────────────────\n",
      "белуджей\".\n",
      "────────  \n",
      "В общей сложности жертвами терактов стали не менее 115 человек, \n",
      "несколько сотен человек получили травмы. \n"
     ]
    }
   ],
   "source": [
    "doc = Doc(texts[0])\n",
    "doc.segment(segmenter)\n",
    "\n",
    "doc.tag_ner(ner_tagger)\n",
    "doc.ner.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__prepare data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сборщик эмбеддингов соседних токенов\n",
    "def neighbours(arr, *, n=1):\n",
    "    length = arr.shape\n",
    "    for i in range(length[0]):\n",
    "        lpad = n - i\n",
    "        rpad = n - (length[0] - i - 1)\n",
    "        if lpad > 0:\n",
    "            res = np.hstack([np.zeros(length[1] * lpad, dtype=arr.dtype), *arr[:i+n+1]])\n",
    "        if rpad > 0:\n",
    "            res = np.hstack([*arr[i-n:], np.zeros(length[1] * rpad, dtype=arr.dtype)])\n",
    "        if (lpad <= 0) and (rpad <= 0):\n",
    "            res = np.hstack([*arr[i-n:i+n+1]])\n",
    "        yield res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "      <th>word_idx</th>\n",
       "      <th>tag_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Министра</td>\n",
       "      <td>OUT</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Белуджистана</td>\n",
       "      <td>LOC</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>отправили</td>\n",
       "      <td>OUT</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>в</td>\n",
       "      <td>OUT</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>отставку</td>\n",
       "      <td>OUT</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sent          word  tag  word_idx  tag_idx\n",
       "0     0      Министра  OUT         0        4\n",
       "1     0  Белуджистана  LOC         1        1\n",
       "2     0     отправили  OUT         2        4\n",
       "3     0             в  OUT         3        4\n",
       "4     0      отставку  OUT         4        4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VEC_SIZE = 500\n",
    "\n",
    "# embeddings\n",
    "sentences = data.groupby('sent')['word'].agg(list)\n",
    "ft = FastText(sentences, vector_size=VEC_SIZE, min_count=5)\n",
    "embs = np.array(data['word'].apply(lambda val: ft.wv[val]).tolist())\n",
    "embs = np.array(list(neighbours(embs, n=2)))      # собрать по 2 токена с каждой стороны\n",
    "\n",
    "# encode tokens\n",
    "VOCAB_SIZE = data['word'].unique().size\n",
    "idx2word = dict(enumerate(data['word'].unique()))\n",
    "word2idx = {v: k for k, v in idx2word.items()}\n",
    "data['word_idx'] = data['word'].map(word2idx)\n",
    "\n",
    "# encode labels\n",
    "enc = LabelEncoder()\n",
    "data['tag_idx'] = enc.fit_transform(data['tag'])\n",
    "labels = np.eye(enc.classes_.size)[data['tag_idx'].values]\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# vectorizer = CountVectorizer(ngram_range=(1, 3), analyzer='word', max_df=0.9, max_features=2500)\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), analyzer='word', max_df=0.9, max_features=2500)\n",
    "cnvec = vectorizer.fit_transform(data['word']).astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__custom NER__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.22      0.24       271\n",
      "           1       0.09      0.01      0.02       120\n",
      "           2       0.00      0.00      0.00        13\n",
      "           3       0.30      0.19      0.23       327\n",
      "           4       0.98      0.99      0.99     50900\n",
      "           5       0.20      0.16      0.17       318\n",
      "\n",
      "    accuracy                           0.97     51949\n",
      "   macro avg       0.31      0.26      0.28     51949\n",
      "weighted avg       0.97      0.97      0.97     51949\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fit\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(embs, data['tag_idx'].values, test_size=0.2, shuffle=False)\n",
    "\n",
    "model = LGBMClassifier(n_estimators=500, num_leaves=29, random_state=19, n_jobs=-1)\n",
    "model.fit(train_x, train_y)\n",
    "\n",
    "# evaluate\n",
    "predicts = model.predict(valid_x)\n",
    "print(classification_report(valid_y, predicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.14      0.24       271\n",
      "           1       0.44      0.12      0.18       120\n",
      "           2       0.00      0.00      0.00        13\n",
      "           3       0.68      0.30      0.42       327\n",
      "           4       0.98      1.00      0.99     50900\n",
      "           5       0.36      0.03      0.06       318\n",
      "\n",
      "    accuracy                           0.98     51949\n",
      "   macro avg       0.54      0.27      0.32     51949\n",
      "weighted avg       0.97      0.98      0.97     51949\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fit\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(cnvec, data['tag_idx'].values, test_size=0.2, shuffle=False)\n",
    "\n",
    "model = LGBMClassifier(n_estimators=500, num_leaves=29, random_state=19, n_jobs=-1)\n",
    "model.fit(train_x, train_y)\n",
    "\n",
    "# evaluate\n",
    "predicts = model.predict(valid_x)\n",
    "print(classification_report(valid_y, predicts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NN keras__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layer\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-22 20:31:27.003539: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-22 20:31:27.198429: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-10-22 20:31:27.198464: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-10-22 20:31:27.229857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# train/valid split\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(data['word'].values, labels, test_size=0.2, shuffle=False)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y)).batch(BATCH_SIZE).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "valid_data = tf.data.Dataset.from_tensor_slices((valid_x, valid_y)).batch(BATCH_SIZE).cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    return input_data\n",
    "\n",
    "# VOCAB_SIZE = 30000\n",
    "SEQ_LEN = 32\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    #ngrams=(1, 3),\n",
    "    output_sequence_length=SEQ_LEN)\n",
    "\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "text_data = train_data.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasNER(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embdim, hdim, out):\n",
    "        super().__init__()\n",
    "        self.emb = layer.Embedding(vocab_size, embdim)\n",
    "        self.gPool = layer.GlobalMaxPooling1D()\n",
    "        self.fc1 = layer.Dense(2 * hdim, activation='relu')\n",
    "        self.fc2 = layer.Dense(hdim, activation='relu')\n",
    "        self.fc3 = layer.Dense(out, activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = vectorize_layer(x)\n",
    "        x = self.emb(x)\n",
    "        pool_x = self.gPool(x)\n",
    "        \n",
    "        fc_x = self.fc1(pool_x)\n",
    "        fc_x = self.fc2(fc_x)\n",
    "        \n",
    "        concat_x = tf.concat([pool_x, fc_x], axis=1)\n",
    "        prob = self.fc3(concat_x)\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class KerasNER(tf.keras.Model):\n",
    "#     def __init__(self, vocab_size, embdim, hdim, out, layers=2):\n",
    "#         super().__init__()\n",
    "#         self.emb = layer.Embedding(vocab_size, embdim)\n",
    "#         self.lstm = layer.LSTM(layers)\n",
    "#         self.fc1 = layer.Dense(2 * hdim, activation='relu')\n",
    "#         self.fc2 = layer.Dense(hdim, activation='relu')\n",
    "#         self.fc3 = layer.Dense(out, activation='softmax')\n",
    "\n",
    "#     def call(self, x):\n",
    "#         x = vectorize_layer(x)\n",
    "#         x = self.emb(x)\n",
    "#         x_lstm = self.lstm(x)\n",
    "\n",
    "#         x_fc = self.fc1(x_lstm)\n",
    "#         x_fc = self.fc2(x_fc)\n",
    "#         x_cat = tf.concat([x_lstm, x_fc], axis=1)\n",
    "#         prob = self.fc3(x_cat)\n",
    "#         return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-22 20:42:35.162807: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 36549632 exceeds 10% of free system memory.\n",
      "2022-10-22 20:42:35.178033: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 36549632 exceeds 10% of free system memory.\n",
      "2022-10-22 20:42:35.205864: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 36549632 exceeds 10% of free system memory.\n",
      "2022-10-22 20:42:35.342599: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 36549632 exceeds 10% of free system memory.\n",
      "2022-10-22 20:42:35.349021: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 36549632 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3247/3247 [==============================] - 359s 110ms/step - loss: 0.0877 - precision: 0.9847 - val_loss: 0.0942 - val_precision: 0.9822\n",
      "Epoch 2/3\n",
      "3247/3247 [==============================] - 355s 109ms/step - loss: 0.0324 - precision: 0.9908 - val_loss: 0.1365 - val_precision: 0.9797\n",
      "Epoch 3/3\n",
      "3247/3247 [==============================] - 358s 110ms/step - loss: 0.0226 - precision: 0.9905 - val_loss: 0.1813 - val_precision: 0.9798\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0b8c3338e0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = KerasNER(VOCAB_SIZE, embdim=256, hdim=128, out=enc.classes_.size)\n",
    "# model = KerasNER(VOCAB_SIZE, embdim=256, hdim=128, out=enc.classes_.size, layers=3)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=['Precision'])\n",
    "model.fit(train_data, validation_data=valid_data, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "812/812 [==============================] - 2s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.17      0.26       271\n",
      "           1       0.36      0.40      0.38       120\n",
      "           2       0.00      0.00      0.00        13\n",
      "           3       0.54      0.37      0.44       327\n",
      "           4       0.98      0.99      0.99     50900\n",
      "           5       0.50      0.16      0.24       318\n",
      "\n",
      "    accuracy                           0.98     51949\n",
      "   macro avg       0.49      0.35      0.38     51949\n",
      "weighted avg       0.98      0.98      0.98     51949\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avagadro/anaconda3/envs/jupyter_default/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/avagadro/anaconda3/envs/jupyter_default/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/avagadro/anaconda3/envs/jupyter_default/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# v1: pool\n",
    "predicts = model.predict(valid_data)\n",
    "print(classification_report(valid_y.argmax(axis=1), predicts.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NN torch__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не работает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from common import TaggerDataset, TorchTrainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(torch.nn.Module, TorchTrainable):\n",
    "#     def __init__(self, vocab_size, dim, out, drop=0.2):\n",
    "#         super().__init__()\n",
    "#         self.emb = torch.nn.Embedding(vocab_size, 2 * dim)\n",
    "#         self.pool = torch.nn.MaxPool1d(2)\n",
    "#         self.fc1 = torch.nn.Linear(dim, 2 * dim)\n",
    "#         self.fc2 = torch.nn.Linear(2 * dim, dim)\n",
    "#         self.fc3 = torch.nn.Linear(2 * dim, out)\n",
    "#         self.dp = torch.nn.Dropout(drop)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.emb(x)\n",
    "#         pool_x = self.pool(x)\n",
    "        \n",
    "#         fc_x = self.fc1(pool_x)\n",
    "#         fc_x = torch.relu(fc_x)\n",
    "#         fc_x = self.fc2(fc_x)\n",
    "#         fc_x = torch.relu(fc_x)\n",
    "#         concat_x = torch.cat([pool_x, fc_x], axis=1)\n",
    "        \n",
    "#         x = self.fc3(concat_x)\n",
    "#         x = torch.softmax(x, dim=1)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module, TorchTrainable):\n",
    "    def __init__(self, vocab_size, inp, dim, out, drop=0.2):\n",
    "        super().__init__()\n",
    "        self.pool = torch.nn.AvgPool1d(2)\n",
    "        self.fc1 = torch.nn.Linear(inp // 2, 2 * dim)\n",
    "        self.fc2 = torch.nn.Linear(2 * dim, dim)\n",
    "        self.fc3 = torch.nn.Linear(dim + inp // 2, out)\n",
    "        self.dp = torch.nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_pool = self.pool(x)\n",
    "        x_fc = self.fc1(x_pool)\n",
    "        x_fc = torch.relu(x_fc)\n",
    "        x_fc = self.fc2(x_fc)\n",
    "        x_fc = torch.relu(x_fc)\n",
    "        concat_x = torch.cat([x_pool, x_fc], axis=1)\n",
    "        \n",
    "        x = self.fc3(concat_x)\n",
    "        x = torch.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(torch.nn.Module, TorchTrainable):\n",
    "#     def __init__(self, vocab_size, dim, out, drop=0.2, layers=2, avg=torch.mean):\n",
    "#         super().__init__()\n",
    "#         self.emb = torch.nn.Embedding(vocab_size, 2 * dim)        \n",
    "#         self.lstm = torch.nn.LSTM(2 * dim, dim, num_layers=layers, batch_first=True, bidirectional=True, dropout=drop)\n",
    "#         # self.gru = torch.nn.GRU(2 * dim, dim, num_layers=layers, batch_first=True, bidirectional=True, dropout=drop)\n",
    "#         self.avg = avg\n",
    "#         self.linear = torch.nn.Linear(2 * dim, out)\n",
    "#         self.dp = torch.nn.Dropout(drop)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.emb(x)\n",
    "#         x = self.dp(x)\n",
    "#         x, ht = self.lstm(x)\n",
    "#         # x, ht = self.gru(x)\n",
    "#         # x = x[:, -1, :] if self.avg is None else self.avg(x, dim=1)\n",
    "#         x = self.linear(x)\n",
    "#         x = torch.softmax(x, dim=0)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x, valid_x, train_y, valid_y = train_test_split(data['word_idx'].values, labels, test_size=0.2, shuffle=False)\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(cnvec.toarray(), labels, test_size=0.2, shuffle=False)\n",
    "# train_x, valid_x, train_y, valid_y = train_test_split(embs, labels, test_size=0.2, shuffle=False)\n",
    "\n",
    "# VOCAB_SIZE = 30000\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "train_dataset = TaggerDataset(train_x, train_y, dtype=torch.float)\n",
    "valid_dataset = TaggerDataset(valid_x, valid_y, dtype=torch.float)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 812/812 [00:20<00:00, 40.28it/s, cumulative loss per item=0.00415]\n",
      "Epoch 2/3: 100%|██████████| 812/812 [00:08<00:00, 96.19it/s, cumulative loss per item=0.00414]\n",
      "Epoch 3/3: 100%|██████████| 812/812 [00:12<00:00, 65.68it/s, cumulative loss per item=0.00414]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# fit\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Selected device: {device}')\n",
    "# net = Net(VOCAB_SIZE, dim=512, out=enc.classes_.size, drop=0.1).to(device)\n",
    "net = Net(VOCAB_SIZE, inp=2500, dim=512, out=enc.classes_.size, drop=0.1).to(device)\n",
    "# net = Net(VOCAB_SIZE, dim=512, out=enc.classes_.size, drop=0.1, layers=3).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "net.fit(train_loader, optimizer, criterion, epochs=3, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       271\n",
      "           1       0.00      0.00      0.00       120\n",
      "           2       0.00      0.00      0.00        13\n",
      "           3       0.00      0.00      0.00       327\n",
      "           4       0.98      1.00      0.99     50900\n",
      "           5       0.00      0.00      0.00       318\n",
      "\n",
      "    accuracy                           0.98     51949\n",
      "   macro avg       0.16      0.17      0.16     51949\n",
      "weighted avg       0.96      0.98      0.97     51949\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avagadro/anaconda3/envs/jupyter_default/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/avagadro/anaconda3/envs/jupyter_default/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/avagadro/anaconda3/envs/jupyter_default/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "predicts = net.predict(valid_loader)\n",
    "lb_pred = predicts.argmax(axis=1)\n",
    "print(classification_report(valid_y.argmax(axis=1), lb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('jupyter_default')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "23a8a6843721b26098060b435da282c6499d0f0384483463012990926fcfc80c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
