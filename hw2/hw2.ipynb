{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from html import unescape\n",
    "from functools import partial\n",
    "from string import punctuation\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__load previous data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    29720\n",
       "1     2242\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apostrophe_dict = json.load(open('../hw1/data/apostrophe.json', 'r', encoding='utf-8'))\n",
    "short_word_dict = json.load(open('../hw1/data/short_words.json', 'r', encoding='utf-8'))\n",
    "emoticon_dict = json.load(open('../hw1/data/emotions.json', 'r', encoding='utf-8'))\n",
    "\n",
    "data = pd.read_csv('../hw1/data/train_tweets.csv')\n",
    "data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__used functions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# взято из hw1\n",
    "def remap_func(text, dictionary):\n",
    "    return ' '.join([dictionary.get(w, w) for w in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# собрано из hw1\n",
    "def data_clean(data):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    lemm = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "    data = data.copy()\n",
    "    data = data.apply(unescape)\n",
    "    data = data.str.replace(r'@\\w*', '', regex=True)\n",
    "    data = data.str.lower()\n",
    "    data = data.apply(partial(remap_func, dictionary=apostrophe_dict))\n",
    "    data = data.apply(partial(remap_func, dictionary=short_word_dict))\n",
    "    data = data.apply(partial(remap_func, dictionary=emoticon_dict))\n",
    "    data = data.str.replace(f'[{punctuation}0-9]', ' ', regex=True)\n",
    "    data = data.str.replace(r'\\b\\S\\b', '', regex=True)\n",
    "    data = data.apply(nltk.tokenize.word_tokenize)\n",
    "    data = data.apply(lambda tokens: [w for w in tokens if w not in stopwords])\n",
    "    data = data.apply(lambda tokens: [lemm.lemmatize(w) for w in tokens])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>[father, dysfunctional, selfish, drag, kid, dy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>[thanks, lyft, credit, use, cause, offer, whee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>[bihday, majesty]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>[model, love, take, time, urð±, ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>[factsguide, society, motivation]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  \\\n",
       "0   1      0   @user when a father is dysfunctional and is s...   \n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...   \n",
       "2   3      0                                bihday your majesty   \n",
       "3   4      0  #model   i love u take with u all the time in ...   \n",
       "4   5      0             factsguide: society now    #motivation   \n",
       "\n",
       "                                               lemma  \n",
       "0  [father, dysfunctional, selfish, drag, kid, dy...  \n",
       "1  [thanks, lyft, credit, use, cause, offer, whee...  \n",
       "2                                  [bihday, majesty]  \n",
       "3  [model, love, take, time, urð±, ,...  \n",
       "4                  [factsguide, society, motivation]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare train data\n",
    "data['lemma'] = data_clean(data['tweet'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__vectorizers__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "corpus = data['lemma'].str.join(' ')\n",
    "\n",
    "\n",
    "class DataKeeper:\n",
    "    def __init__(self, corpus, **models):\n",
    "        self.corpus = corpus\n",
    "        self.models = models\n",
    "        self.fields = []\n",
    "        self.fitted = []\n",
    "    \n",
    "    def prepare(self):\n",
    "        for name, model in self.models.items():\n",
    "            model.fit(self.corpus)\n",
    "            matrix = model.transform(self.corpus)\n",
    "            mtx_name, mdl_name = f'matrix_{name}', f'model_{name}'\n",
    "            setattr(self, mtx_name, matrix)\n",
    "            setattr(self, mdl_name, model)\n",
    "            self.fields.append(mtx_name)\n",
    "            self.fitted.append(mdl_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# подготовка матриц\n",
    "keeper = DataKeeper(corpus,\n",
    "    cnvec_high=CountVectorizer(stop_words='english', max_features=1000, min_df=1, max_df=0.9),\n",
    "    tfidf_high=TfidfVectorizer(stop_words='english', max_features=1000, min_df=1, max_df=0.9),\n",
    "    cnvec_mid=CountVectorizer(stop_words='english', max_features=1000, min_df=1e-3, max_df=0.7),\n",
    "    tfidf_mid=TfidfVectorizer(stop_words='english', max_features=1000, min_df=1e-3, max_df=0.7),\n",
    "    cnvec_low=CountVectorizer(stop_words='english', max_features=1000, min_df=1e-5, max_df=0.3),\n",
    "    tfidf_low=TfidfVectorizer(stop_words='english', max_features=1000, min_df=1e-5, max_df=0.3),\n",
    "    hashing=HashingVectorizer(n_features=1000),\n",
    ")\n",
    "keeper.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>matrix_cnvec_high</th>\n",
       "      <td>0.822866</td>\n",
       "      <td>0.325597</td>\n",
       "      <td>0.466220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matrix_tfidf_high</th>\n",
       "      <td>0.847976</td>\n",
       "      <td>0.273854</td>\n",
       "      <td>0.413852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matrix_cnvec_mid</th>\n",
       "      <td>0.821968</td>\n",
       "      <td>0.325597</td>\n",
       "      <td>0.466094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matrix_tfidf_mid</th>\n",
       "      <td>0.849493</td>\n",
       "      <td>0.274746</td>\n",
       "      <td>0.415084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matrix_cnvec_low</th>\n",
       "      <td>0.822866</td>\n",
       "      <td>0.325597</td>\n",
       "      <td>0.466220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matrix_tfidf_low</th>\n",
       "      <td>0.847976</td>\n",
       "      <td>0.273854</td>\n",
       "      <td>0.413852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matrix_hashing</th>\n",
       "      <td>0.947595</td>\n",
       "      <td>0.075821</td>\n",
       "      <td>0.140269</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   precision    recall        f1\n",
       "matrix_cnvec_high   0.822866  0.325597  0.466220\n",
       "matrix_tfidf_high   0.847976  0.273854  0.413852\n",
       "matrix_cnvec_mid    0.821968  0.325597  0.466094\n",
       "matrix_tfidf_mid    0.849493  0.274746  0.415084\n",
       "matrix_cnvec_low    0.822866  0.325597  0.466220\n",
       "matrix_tfidf_low    0.847976  0.273854  0.413852\n",
       "matrix_hashing      0.947595  0.075821  0.140269"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# обучение и сравнение классификаторов\n",
    "reports = {}\n",
    "for fname in keeper.fields:\n",
    "    matrix = getattr(keeper, fname)\n",
    "    precision, recall, f1 = [], [], []\n",
    "    # train/valid split    \n",
    "    skf = StratifiedKFold(3, shuffle=True, random_state=11)\n",
    "    for train, valid in skf.split(matrix, data['label']):\n",
    "        # model = SGDClassifier(learning_rate='adaptive', eta0=0.01, class_weight='balanced', random_state=19)\n",
    "        model = SGDClassifier(learning_rate='adaptive', eta0=0.01, random_state=19)\n",
    "        # model = LGBMClassifier(class_weight='balanced', random_state=19)\n",
    "        # model = LGBMClassifier(random_state=19)\n",
    "        model.fit(matrix[train].astype(np.float32), data.loc[train, 'label'])\n",
    "        predicts = model.predict(matrix[valid].astype(np.float32))\n",
    "        # metrics & report\n",
    "        precision.append(precision_score(data.loc[valid, 'label'], predicts))\n",
    "        recall.append(recall_score(data.loc[valid, 'label'], predicts))\n",
    "        f1.append(f1_score(data.loc[valid, 'label'], predicts))\n",
    "        reports[fname] = [np.mean(precision), np.mean(recall), np.mean(f1)]\n",
    "\n",
    "pd.DataFrame(reports, index=['precision', 'recall', 'f1']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__feature importance__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      5945\n",
      "           1       0.86      0.26      0.40       448\n",
      "\n",
      "    accuracy                           0.95      6393\n",
      "   macro avg       0.91      0.63      0.68      6393\n",
      "weighted avg       0.94      0.95      0.93      6393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take one model\n",
    "matrix = keeper.matrix_tfidf_mid\n",
    "train, valid = train_test_split(np.arange(matrix.shape[0]), test_size=0.2, stratify=data['label'], shuffle=True, random_state=11)\n",
    "\n",
    "# model = LGBMClassifier(random_state=19)\n",
    "model = SGDClassifier(learning_rate='adaptive', eta0=0.01, random_state=19)\n",
    "\n",
    "model.fit(matrix[train], data.loc[train, 'label'])\n",
    "predicts = model.predict(matrix[valid])\n",
    "print(classification_report(data.loc[valid, 'label'], predicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read more: https://towardsdatascience.com/boruta-explained-the-way-i-wish-someone-explained-it-to-me-4489d70e154a\n",
    "class SGDFeatureSelector:\n",
    "    def __init__(self, model, n_estimators=100, random_state=None):\n",
    "        self.model = model\n",
    "        self.n_estimators = n_estimators\n",
    "        self.randomizer = np.random.default_rng(random_state)\n",
    "        self.hits = None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.hits = np.zeros(X.shape[1])        \n",
    "        for n in tqdm(range(self.n_estimators), total=self.n_estimators, desc='Fitting'):\n",
    "            # extend X with shadows - вот тут хорошо бы не переходить к dense-матрице, но реализация требует времени\n",
    "            shadows = np.apply_along_axis(np.random.permutation, 1, X.toarray())\n",
    "            X_extended = np.hstack([X.toarray(), shadows])\n",
    "            self.model.fit(X_extended, y)\n",
    "            # get importances\n",
    "            importances = self.model.coef_.flatten()\n",
    "            threshold = importances[importances.size // 2:].max()\n",
    "            self.hits += importances[:importances.size // 2] > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting: 100%|██████████| 10/10 [00:48<00:00,  4.84s/it]\n"
     ]
    }
   ],
   "source": [
    "# обучение\n",
    "fs = SGDFeatureSelector(model, n_estimators=10, random_state=17)\n",
    "fs.fit(matrix[train], data.loc[train, 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['able', 'pop', 'positive', 'positivity', 'post', 'power', 'pray',\n",
       "       'prayer', 'prayfororlando', 'praying', 'pre', 'present', 'pretty',\n",
       "       'poor', 'previous', 'pride', 'probably', 'problem', 'product',\n",
       "       'project', 'proud', 'public', 'pulse', 'punjab', 'puppy', 'pussy',\n",
       "       'queen', 'price', 'question', 'pool', 'political', 'peace',\n",
       "       'people', 'perfect', 'person', 'pet', 'phone', 'photo',\n",
       "       'photography', 'photooftheday', 'pic', 'picoftheday', 'picture',\n",
       "       'politician', 'piece', 'pizza', 'place', 'plan', 'planning',\n",
       "       'play'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = fs.hits.argsort()[:50]\n",
    "keeper.model_tfidf_mid.get_feature_names_out()[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NN approach__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset():\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cell(torch.nn.Module):\n",
    "    def __init__(self, inp, out, *, drop=0):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(inp, out)\n",
    "        self.bn = torch.nn.BatchNorm1d(out)\n",
    "        self.dp = torch.nn.Dropout(drop) if drop else None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.bn(x)\n",
    "        if self.dp is not None:\n",
    "            x = self.dp(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cell1 = Cell(1000, 512, drop=0.2)\n",
    "        self.cell2 = Cell(512, 128, drop=0.2)\n",
    "        self.cell3 = Cell(128, 1, drop=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cell1(x)\n",
    "        x = self.cell2(x)\n",
    "        x = self.cell3(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 400/400 [00:03<00:00, 113.99it/s, cumulative loss per item=0.00938]\n",
      "Epoch 2/5: 100%|██████████| 400/400 [00:03<00:00, 113.14it/s, cumulative loss per item=0.00718]\n",
      "Epoch 3/5: 100%|██████████| 400/400 [00:03<00:00, 116.79it/s, cumulative loss per item=0.00606]\n",
      "Epoch 4/5: 100%|██████████| 400/400 [00:03<00:00, 115.66it/s, cumulative loss per item=0.00526]\n",
      "Epoch 5/5: 100%|██████████| 400/400 [00:03<00:00, 115.03it/s, cumulative loss per item=0.00476]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "matrix = keeper.matrix_tfidf_mid\n",
    "\n",
    "# init network\n",
    "device = 'cpu'      # на gpu не работает\n",
    "net = Net().to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "# make data loader\n",
    "tweetset = TweetDataset(matrix[train].toarray(), data.loc[train, 'label'].reset_index(drop=True))\n",
    "loader = torch.utils.data.DataLoader(tweetset, batch_size=64, shuffle=True)\n",
    "\n",
    "# train\n",
    "net.train()\n",
    "for ep in range(EPOCHS):\n",
    "    sum_loss, items = 0.0, 0\n",
    "    pbar = tqdm(enumerate(loader), total=len(loader), desc=f'Epoch {ep + 1}/{EPOCHS}')\n",
    "    for i, batch in pbar:\n",
    "        inputs, labels = batch[0].to(device).float(), batch[1].to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs.flatten(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        sum_loss += loss.item()\n",
    "        items += len(labels)\n",
    "        pbar.set_postfix({'cumulative loss per item': sum_loss / items})\n",
    "print('\\nDone.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97      5945\n",
      "           1       0.72      0.40      0.52       448\n",
      "\n",
      "    accuracy                           0.95      6393\n",
      "   macro avg       0.84      0.70      0.74      6393\n",
      "weighted avg       0.94      0.95      0.94      6393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "predicts = net(torch.FloatTensor(matrix[valid].toarray())).detach().numpy() > 0.5\n",
    "print(classification_report(data.loc[valid, 'label'], predicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('jupyter_default')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "23a8a6843721b26098060b435da282c6499d0f0384483463012990926fcfc80c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
