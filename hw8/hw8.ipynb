{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from string import punctuation\n",
    "from functools import partial\n",
    "from itertools import chain\n",
    "\n",
    "import nltk\n",
    "import pymorphy2\n",
    "from pymorphy2.tokenizers import simple_word_tokenize\n",
    "from joblib import Parallel, parallel_backend, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Selected device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/avagadro/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "STOPWORDS = nltk.corpus.stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__read & clean data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train = pd.read_csv('data/train.csv', index_col='id')\n",
    "raw_valid = pd.read_csv('data/val.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    92063\n",
       "0    89404\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    11449\n",
       "0    11234\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_valid['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lemmatizing: 100%|██████████| 9073/9073 [04:27<00:00, 33.86it/s]\n",
      "dropping stopwords: 100%|██████████| 9073/9073 [00:00<00:00, 45484.28it/s]\n",
      "lemmatizing: 100%|██████████| 1134/1134 [00:17<00:00, 63.40it/s]\n",
      "dropping stopwords: 100%|██████████| 1134/1134 [00:00<00:00, 17953.81it/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(x: pd.Series, morph=pymorphy2.MorphAnalyzer()):\n",
    "    index = x.index.copy()\n",
    "    x = x.copy()\n",
    "    # clean\n",
    "    x = x.str.lower()\n",
    "    x = x.str.replace(r'(@\\S*)\\b', ' ', regex=True)     # remove usernames\n",
    "    x = x.str.replace(r'(http\\S*)\\b', ' ', regex=True)\n",
    "    smiles = x.str.extractall(rf'((?::|;)[{punctuation}]+)\\s').droplevel(1).groupby('id')[0].agg(list)\n",
    "    x = x.str.replace(rf'[{punctuation}]', ' ', regex=True)\n",
    "    x = x.str.replace(r'\\s+', ' ', regex=True)\n",
    "    # tokenize\n",
    "    x = x.apply(simple_word_tokenize)\n",
    "    # lemmatize\n",
    "    with parallel_backend('loky'):\n",
    "        x = Parallel()(delayed(lambda tk: [morph.parse(t)[0].normal_form for t in tk])(item) for item in tqdm(x, desc='lemmatizing'))\n",
    "    # drop stopwords\n",
    "    with parallel_backend('loky'):\n",
    "        x = Parallel()(delayed(lambda tk: [t for t in tk if t not in STOPWORDS])(item) for item in tqdm(x, desc='dropping stopwords'))\n",
    "    # add smiles back\n",
    "    return pd.Series(x, index=index)\n",
    "\n",
    "# take part for increasing development\n",
    "np.random.seed(11)\n",
    "FRAC = 0.05\n",
    "train_tokens = tokenize(raw_train['text'].sample(frac=FRAC, replace=False))\n",
    "valid_tokens = tokenize(raw_valid['text'].sample(frac=FRAC, replace=False))\n",
    "\n",
    "train_labels = raw_train.loc[train_tokens.index, 'class']\n",
    "valid_labels = raw_valid.loc[valid_tokens.index, 'class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15371, 64988)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens = pd.Series(chain(*train_tokens))\n",
    "all_tokens.nunique(), all_tokens.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x180 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(5, 2.5))\n",
    "train_tokens.apply(len).hist(ax=ax[0], edgecolor='black')\n",
    "valid_tokens.apply(len).hist(ax=ax[1], edgecolor='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vectorizing: 100%|██████████| 9073/9073 [00:02<00:00, 3564.53it/s] \n",
      "vectorizing: 100%|██████████| 1134/1134 [00:00<00:00, 1894.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# build tokens dictionary\n",
    "class DictVectorizer:\n",
    "    def __init__(self, vecsize, sticky_edge='start'):\n",
    "        self.id2token = None\n",
    "        self.token2id = None\n",
    "        self.vecsize = vecsize\n",
    "        self.sticky_edge = sticky_edge\n",
    "\n",
    "    def vectorize_item(self, tokens):\n",
    "        vector = [self.token2id[tk] for tk in tokens if tk in self.token2id]          # vectorize\n",
    "        if len(vector) > self.vecsize:\n",
    "            return vector[:self.vecsize] if self.sticky_edge == 'start' else vector[-self.vecsize:]\n",
    "        vector.extend([0] * (self.vecsize - len(vector)))\n",
    "        return vector\n",
    "    \n",
    "    def fit(self, X, dictsize='full'):\n",
    "        all_tokens = [token for row in X for token in row]\n",
    "        freqdist = nltk.probability.FreqDist(all_tokens)\n",
    "        filtered_tokens = [token for token, _ in freqdist.most_common(len(freqdist) if dictsize == 'full' else dictsize)]\n",
    "        self.id2token = dict(enumerate(filtered_tokens))\n",
    "        self.token2id = {v: k for k, v in self.id2token.items()}\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.Series):\n",
    "        with parallel_backend('loky'):\n",
    "            res = Parallel()(delayed(self.vectorize_item)(item) for item in tqdm(X, desc='vectorizing'))\n",
    "        return pd.Series(res, index=X.index)\n",
    "\n",
    "VEC_SIZE = 10\n",
    "dictionary = DictVectorizer(vecsize=VEC_SIZE).fit(train_tokens)\n",
    "train = dictionary.transform(train_tokens)\n",
    "valid = dictionary.transform(valid_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNLPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, vecs, lbs):\n",
    "        self.vecs = vecs\n",
    "        self.lbs = lbs\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # return torch.as_tensor(self.vecs[index]), torch.as_tensor(self.lbs[index].reshape(1, -1))\n",
    "        return torch.as_tensor(self.vecs[index]), torch.as_tensor(self.lbs[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.vecs.shape[0]\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "# make datasets\n",
    "train_dataset = SimpleNLPDataset(train.values, train_labels.values)\n",
    "valid_dataset = SimpleNLPDataset(valid.values, valid_labels.values)\n",
    "# make loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainable:\n",
    "    def fit(self, loader, optim, crit, *, epochs=5, dev='cpu', eval_params=None):\n",
    "        \"\"\"\n",
    "        :param loader - data loader\n",
    "        :param optim - optimizer\n",
    "        :param crit - criterion\n",
    "        :param epochs\n",
    "        :param device\n",
    "        :param eval_params - predict() parameters for evaluation\n",
    "        \"\"\"\n",
    "        self.crit = crit\n",
    "        self.dev = dev\n",
    "        if not eval_params:\n",
    "            eval_params = dict()\n",
    "        self.train()\n",
    "        \n",
    "        for ep in range(epochs):\n",
    "            sum_loss, items = 0.0, 0\n",
    "            pbar = tqdm(enumerate(loader), total=len(loader), desc=f'Epoch {ep + 1}/{epochs}')\n",
    "            for i, batch in pbar:\n",
    "                inputs, labels = batch[0].to(dev), batch[1].to(dev)\n",
    "                optim.zero_grad()\n",
    "                outputs = self(inputs)\n",
    "                loss = crit(outputs, labels.float())\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "\n",
    "                sum_loss += loss.item()\n",
    "                items += len(labels)\n",
    "                pbar.set_postfix({'cumulative loss per item': sum_loss / items})\n",
    "\n",
    "                # evaluate\n",
    "                if (i + 1 == len(loader)) and eval_params:\n",
    "                    self.eval()\n",
    "                    valid_loader = eval_params.get('valid_loader')\n",
    "                    pbar.set_postfix_str('calculating final loss...')\n",
    "\n",
    "                    train_result = self.predict_(loader, **eval_params)\n",
    "                    report = {'loss': train_result[1], 'metric': 'n/a' if len(train_result) == 2 else train_result[2]}\n",
    "\n",
    "                    if valid_loader is not None:\n",
    "                        valid_result = self.predict_(valid_loader, **eval_params)\n",
    "                        report = {'loss': f'{train_result[1]:03f}/{valid_result[1]:03f}',\n",
    "                                  'metric': 'n/a' if len(train_result) == 2 else f'{train_result[2]:03f}/{valid_result[2]:03f}'}\n",
    "                    pbar.set_postfix(report)\n",
    "                    self.train()\n",
    "        print('\\nDone.')\n",
    "\n",
    "    def predict_(self, loader, *, metric=None, threshold=0.5, **kwargs):\n",
    "        if not hasattr(self, 'dev'):\n",
    "            raise AttributeError('Model is not trained.')\n",
    "        self.eval()\n",
    "        loss = 0\n",
    "        for i, batch in enumerate(loader):\n",
    "            inputs, labels = batch[0].to(self.dev), batch[1].to(self.dev)\n",
    "            outputs = self(inputs)\n",
    "            predicts = torch.cat([predicts, outputs]) if i > 0 else outputs\n",
    "            true_labels = torch.cat([true_labels, labels]) if i > 0 else labels\n",
    "\n",
    "            loss += self.crit(outputs, labels.float()).item() / len(loader)\n",
    "        result = [predicts, loss]\n",
    "        if metric:\n",
    "            pred_labels = torch.squeeze(predicts > threshold) * 1\n",
    "            result.append(metric(true_labels.cpu(), pred_labels.detach().cpu()))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(torch.nn.Module, Trainable):\n",
    "    def __init__(self, dict_size, embedding_dim=128, drop=0.3, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(dict_size, 2 * embedding_dim)\n",
    "        self.conv_1 = torch.nn.Conv1d(2 * embedding_dim, embedding_dim, kernel_size=2)\n",
    "\n",
    "        self.linear = torch.nn.Linear(embedding_dim, num_classes)\n",
    "        self.dp = torch.nn.Dropout(drop)\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv_1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = torch.max_pool1d(x, 2)\n",
    "        x = self.dp(x)\n",
    "        x = torch.max(x, axis=2).values\n",
    "        x = self.linear(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 36/36 [00:00<00:00, 45.01it/s, loss=0.474373/0.688202, metric=0.754767/0.632275]\n",
      "Epoch 2/5: 100%|██████████| 36/36 [00:00<00:00, 53.36it/s, loss=0.124638/0.757900, metric=0.964841/0.664021]\n",
      "Epoch 3/5: 100%|██████████| 36/36 [00:00<00:00, 56.95it/s, loss=0.041721/1.093995, metric=0.986333/0.665785]\n",
      "Epoch 4/5: 100%|██████████| 36/36 [00:00<00:00, 54.23it/s, loss=0.037340/1.725984, metric=0.986113/0.663139]\n",
      "Epoch 5/5: 100%|██████████| 36/36 [00:00<00:00, 56.16it/s, loss=0.011244/1.882303, metric=0.996693/0.654321]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DICT_SIZE = all_tokens.nunique()\n",
    "\n",
    "torch.manual_seed(11)       # just in case\n",
    "convmodel = ConvNet(dict_size=DICT_SIZE, embedding_dim=256, drop=0.05, num_classes=1).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(convmodel.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "eval_params = {\n",
    "    'softmax': True,\n",
    "    'metric': accuracy_score,\n",
    "    'valid_loader': valid_loader\n",
    "}\n",
    "convmodel.fit(train_loader, optimizer, criterion, epochs=5, dev=device, eval_params=eval_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.69      0.66       564\n",
      "           1       0.67      0.62      0.64       570\n",
      "\n",
      "    accuracy                           0.65      1134\n",
      "   macro avg       0.66      0.65      0.65      1134\n",
      "weighted avg       0.66      0.65      0.65      1134\n",
      "\n"
     ]
    }
   ],
   "source": [
    "convpred = (convmodel.predict_(valid_loader)[0] > 0.5) * 1\n",
    "print(classification_report(valid_labels.values, convpred.detach().cpu()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNNet(torch.nn.Module, Trainable):\n",
    "    def __init__(self, dict_size, embedding_dim=128, drop=0.3, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(dict_size, 2 * embedding_dim)\n",
    "        self.rnn_1 = torch.nn.LSTM(2 * embedding_dim, embedding_dim, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        # self.rnn_1 = torch.nn.RNN(2 * embedding_dim, embedding_dim, num_layers=3, batch_first=True, bidirectional=True)\n",
    "        self.linear = torch.nn.Linear(2 * embedding_dim, num_classes)\n",
    "        self.dp = torch.nn.Dropout(drop)\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        x = self.embedding(x)\n",
    "        # print(x.shape)\n",
    "        x, h = self.rnn_1(x)\n",
    "        # x = torch.max_pool1d(x, 2)\n",
    "        x = self.dp(x)\n",
    "        x = torch.max(x, axis=1).values\n",
    "        x = torch.relu(x)\n",
    "        # print(x.shape)\n",
    "        x = self.linear(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x.flatten()\n",
    "        # return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 36/36 [00:01<00:00, 31.38it/s, loss=0.692838/0.692765, metric=0.497851/0.512346]\n",
      "Epoch 2/5: 100%|██████████| 36/36 [00:01<00:00, 31.73it/s, loss=0.692791/0.692765, metric=0.497851/0.512346]\n",
      "Epoch 3/5: 100%|██████████| 36/36 [00:01<00:00, 31.72it/s, loss=0.692818/0.692765, metric=0.497851/0.512346]\n",
      "Epoch 4/5: 100%|██████████| 36/36 [00:01<00:00, 31.59it/s, loss=0.692893/0.692765, metric=0.497851/0.512346]\n",
      "Epoch 5/5: 100%|██████████| 36/36 [00:01<00:00, 31.77it/s, loss=0.692817/0.692765, metric=0.497851/0.512346]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DICT_SIZE = all_tokens.nunique()\n",
    "\n",
    "torch.manual_seed(11)       # just in case\n",
    "rnnmodel = RNNNet(dict_size=DICT_SIZE, embedding_dim=256, drop=0.05, num_classes=1).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(convmodel.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "eval_params = {\n",
    "    'softmax': True,\n",
    "    'metric': accuracy_score,\n",
    "    'valid_loader': valid_loader\n",
    "}\n",
    "rnnmodel.fit(train_loader, optimizer, criterion, epochs=5, dev=device, eval_params=eval_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.69      0.66       564\n",
      "           1       0.67      0.62      0.64       570\n",
      "\n",
      "    accuracy                           0.65      1134\n",
      "   macro avg       0.66      0.65      0.65      1134\n",
      "weighted avg       0.66      0.65      0.65      1134\n",
      "\n"
     ]
    }
   ],
   "source": [
    "convpred = (convmodel.predict_(valid_loader)[0] > 0.5) * 1\n",
    "print(classification_report(valid_labels.values, convpred.detach().cpu()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComboNet(torch.nn.Module, Trainable):\n",
    "    def __init__(self, dict_size, embedding_dim=128, drop=0.3, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(dict_size, 2 * embedding_dim)\n",
    "        self.rnn_1 = torch.nn.LSTM(embedding_dim, embedding_dim, num_layers=3, batch_first=True, bidirectional=True)\n",
    "        # self.rnn_1 = torch.nn.RNN(2 * embedding_dim, embedding_dim, num_layers=3, batch_first=True, bidirectional=True)\n",
    "        self.conv_1 = torch.nn.Conv1d(2 * embedding_dim, embedding_dim, kernel_size=2)\n",
    "        self.linear = torch.nn.Linear(2 * embedding_dim, num_classes)\n",
    "        self.dp = torch.nn.Dropout(drop)\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        x = self.conv_1(x)\n",
    "        x = torch.max_pool1d(x, 2)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dp(x)\n",
    "\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x, h = self.rnn_1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dp(x)\n",
    "\n",
    "        x = self.linear(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = torch.mean(x, axis=1)\n",
    "        return x.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 36/36 [00:01<00:00, 34.81it/s, loss=0.693086/0.693073, metric=0.501488/0.498236]\n",
      "Epoch 2/5: 100%|██████████| 36/36 [00:01<00:00, 32.22it/s, loss=0.693082/0.693073, metric=0.501488/0.498236]\n",
      "Epoch 3/5: 100%|██████████| 36/36 [00:01<00:00, 35.95it/s, loss=0.693085/0.693073, metric=0.501488/0.498236]\n",
      "Epoch 4/5: 100%|██████████| 36/36 [00:01<00:00, 35.12it/s, loss=0.693075/0.693073, metric=0.501488/0.498236]\n",
      "Epoch 5/5: 100%|██████████| 36/36 [00:01<00:00, 35.89it/s, loss=0.693089/0.693073, metric=0.501488/0.498236]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DICT_SIZE = all_tokens.nunique()\n",
    "\n",
    "torch.manual_seed(11)       # just in case\n",
    "combomodel = ComboNet(dict_size=DICT_SIZE, embedding_dim=256, drop=0.05, num_classes=1).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(convmodel.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "eval_params = {\n",
    "    'softmax': True,\n",
    "    'metric': accuracy_score,\n",
    "    'valid_loader': valid_loader\n",
    "}\n",
    "combomodel.fit(train_loader, optimizer, criterion, epochs=5, dev=device, eval_params=eval_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.66       564\n",
      "           1       1.00      0.00      0.00       570\n",
      "\n",
      "    accuracy                           0.50      1134\n",
      "   macro avg       0.75      0.50      0.33      1134\n",
      "weighted avg       0.75      0.50      0.33      1134\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combopred = (combomodel.predict_(valid_loader)[0] > 0.5) * 1\n",
    "print(classification_report(valid_labels.values, combopred.detach().cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "23a8a6843721b26098060b435da282c6499d0f0384483463012990926fcfc80c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
